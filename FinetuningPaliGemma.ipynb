{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "from PIL import Image\n",
    "import io\n",
    "import os\n",
    "from bisect import bisect_left\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from transformers import AutoProcessor, AutoModelForPreTraining\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import get_peft_model, LoraConfig\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Clear transformers cache\n",
    "cache_dir = os.path.expanduser(\"~/.cache/huggingface/hub\")\n",
    "if os.path.exists(cache_dir):\n",
    "    print(f\"Removing cache from {cache_dir}\")\n",
    "    shutil.rmtree(cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "#Add the file-path here\n",
    "# 1. Path to images folder\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 1\n",
    "# 2. Path to test.csv\n",
    "data_dir = \"/teamspace/studios/this_studio/images\" \n",
    "csv_filename = \"/teamspace/studios/this_studio/saved_images.csv\"\n",
    "metadata_df = pd.read_csv(csv_filename)\n",
    "metadata_df[\"image\"] = [x.split('/')[-1] for x in metadata_df[\"image_link\"]]\n",
    "\n",
    "metadata_df = metadata_df.drop(columns=[\"image_link\", \"group_id\"])\n",
    "\n",
    "\n",
    "new_eval = [f'What is the {x}?' for x in metadata_df[\"entity_name\"]]\n",
    "metadata_df[\"entity_name\"] = new_eval\n",
    "\n",
    "# This gives you roughly:\n",
    "# - 80% train\n",
    "# - 10% validation\n",
    "# - 10% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=0\n",
    "def clean_value(value):\n",
    "    value = str(value).strip('[]').strip()\n",
    "    global c\n",
    "    try:\n",
    "        # Split into parts\n",
    "        parts = value.split()\n",
    "        \n",
    "        # Get first number whether it's a range or single number\n",
    "        number_part = parts[0].split(',')[0]  # Take first number if there's a comma\n",
    "        number = float(number_part.replace(',', ''))\n",
    "        \n",
    "        # Get unit if it exists\n",
    "        unit = parts[-1] if len(parts) > 1 else ''\n",
    "        \n",
    "        # Convert to int if it's a whole number\n",
    "        if number.is_integer():\n",
    "            number = int(number)\n",
    "        \n",
    "        # Return formatted string\n",
    "        if unit:\n",
    "            return f\"{number} {unit}\"\n",
    "        return str(number)\n",
    "    except:\n",
    "        print(f\"Could not process: {value}\")\n",
    "        c+=1\n",
    "        return value\n",
    "metadata_df['entity_value']=metadata_df['entity_value'].apply(clean_value)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First split: separate test set\n",
    "train_val_df, test_df = train_test_split(metadata_df, test_size=0.166, random_state=7,stratify=metadata_df['entity_name'],)\n",
    "\n",
    "# Second split: separate validation from train\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.8, random_state=7,stratify=train_val_df['entity_name'],)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5004\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 5004\n"
     ]
    }
   ],
   "source": [
    "class VQADataset(Dataset):\n",
    "    def __init__(self, df_val, img_folder, transform=None):\n",
    "        self.data = df_val\n",
    "        self.img_folder = img_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img_name = self.data.iloc[idx]['image']\n",
    "            prefix = self.data.iloc[idx]['entity_name']\n",
    "            suffix = self.data.iloc[idx]['entity_value']\n",
    "            \n",
    "            img_path = os.path.join(self.img_folder, img_name)\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "                \n",
    "            if image.shape != (3, 448, 448):\n",
    "                print(f\"Skipping image {img_name} due to wrong shape: {image.shape}\")\n",
    "                return None\n",
    "\n",
    "            return {\"image\": image, \"entity_name\": prefix, \"entity_value\": suffix}\n",
    "        except Exception as e:\n",
    "            print(f\"Error with image {img_name}: {str(e)}\")\n",
    "            return None\n",
    "def collate_fn(examples):\n",
    "    # Filter out None values\n",
    "    examples = [ex for ex in examples if ex is not None]\n",
    "    if not examples:\n",
    "        return None\n",
    "    \n",
    "    # Get input texts and images\n",
    "    texts = [example['entity_name'] for example in examples]\n",
    "    images = [example[\"image\"] for example in examples]\n",
    "    \n",
    "    # Get target texts (answers)\n",
    "    target_texts = [example['entity_value'] for example in examples]\n",
    "    \n",
    "    # Process inputs (questions and images)\n",
    "    inputs = processor(\n",
    "        text=texts, \n",
    "        images=images, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=\"longest\",\n",
    "    )\n",
    "    \n",
    "    # Process targets (answers) separately\n",
    "    target_tokens = processor.tokenizer(\n",
    "        target_texts,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    \n",
    "    # Create decoder_input_ids (shifted right)\n",
    "    decoder_input_ids = target_tokens.input_ids.clone()\n",
    "    decoder_input_ids = torch.cat(\n",
    "        [\n",
    "            torch.ones((decoder_input_ids.shape[0], 1), dtype=torch.long) * processor.tokenizer.bos_token_id,\n",
    "            decoder_input_ids[:, :-1]\n",
    "        ],\n",
    "        dim=-1\n",
    "    )\n",
    "    \n",
    "    # Add decoder inputs and labels to the inputs dict\n",
    "    inputs['decoder_input_ids'] = decoder_input_ids\n",
    "    inputs['labels'] = target_tokens.input_ids\n",
    "    device='cuda'\n",
    "    # Move everything to device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    return inputs\n",
    "device = \"cuda\"\n",
    "model_id = \"google/paligemma-3b-ft-docvqa-448\"\n",
    "processor = AutoProcessor.from_pretrained(model_id, do_rescale=False)\n",
    "def collate_fn(examples):\n",
    "    \n",
    "    prefixes = [example['entity_name'] for example in examples]\n",
    "    suffixes = [example['entity_value'] for example in examples]\n",
    "    images = [example[\"image\"] for example in examples]\n",
    "\n",
    "    images = torch.stack(images)\n",
    "    tokens = processor(text=prefixes, images=images, suffix=suffixes,\n",
    "                       return_tensors=\"pt\", padding=\"longest\")\n",
    "\n",
    "    tokens = tokens.to(torch.bfloat16).to(device)\n",
    "\n",
    "    return tokens\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((448, 448)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "#Initializing the dataset\n",
    "train_dataset = VQADataset(train_df, data_dir,  transform)\n",
    "val_dataset = VQADataset(val_df, data_dir,  transform)\n",
    "test_dataset = VQADataset(test_df, data_dir,  transform)\n",
    "dataset_size = len(train_dataset)\n",
    "print(f\"Train size: {dataset_size}\")\n",
    "\n",
    "#Loading the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['bnb_4bit_compute_type', 'skip_modules_not_needed', 'offload_to_cpu']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66f00c08dfbe4e078b779046cc546a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,230,016 || all params: 2,931,576,560 || trainable%: 0.2125\n",
      "None\n",
      "trainable params: 6,230,016 || all params: 2,931,576,560 || trainable%: 0.2125\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "\n",
    "# CHECKPOINT = \"microsoft/Florence-2-base-ft\"\n",
    "# CHECKPOINT='prithivMLmods/Florence-2-VLM-Doc-VQA'\n",
    "# CHECKPOINT='adamchanadam/Test_Florence-2-FT-DocVQA'\n",
    "# CHECKPOINT='microsoft/Florence-2-base'\n",
    "CHECKPOINT=\"google/paligemma-3b-ft-docvqa-448\"\n",
    "REVISION = 'refs/pr/6'\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_type=torch.bfloat16,\n",
    "    skip_modules_not_needed=True,\n",
    "    offload_to_cpu=True,\n",
    ")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     CHECKPOINT, trust_remote_code=True,quantization_config=bnb_config\n",
    "#  ).to(DEVICE)\n",
    "# processor = AutoProcessor.from_pretrained(\n",
    "#     CHECKPOINT, trust_remote_code=True)\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# TARGET_MODULES = [\n",
    "#     \"q_proj\", # Only attention queries\n",
    "#     \"v_proj\", # Only attention values\n",
    "#     \"fc2\"     # FFN down-projections\n",
    "# ]\n",
    "TARGET_MODULES=[\"q_proj\", \"v_proj\",  \"up_proj\", \"down_proj\"]\n",
    "model = AutoModelForPreTraining.from_pretrained(CHECKPOINT, quantization_config=bnb_config, device_map={\"\":0})\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=8,\n",
    "    target_modules=TARGET_MODULES,\n",
    "    task_type=\"CAUSAL_LM\"\n",
    "\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "for param in model.vision_tower.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.multi_modal_projector.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "print(model.print_trainable_parameters())\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madithyabalagoni11\u001b[0m (\u001b[33madithyabalagoni11-vasavi-college-of-engineering\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /teamspace/studios/this_studio/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/teamspace/studios/this_studio/wandb/run-20241028_130102-0l7us3rf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adithyabalagoni11-vasavi-college-of-engineering/paligemma_finetuning-003/runs/0l7us3rf' target=\"_blank\">quiet-tree-1</a></strong> to <a href='https://wandb.ai/adithyabalagoni11-vasavi-college-of-engineering/paligemma_finetuning-003' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adithyabalagoni11-vasavi-college-of-engineering/paligemma_finetuning-003' target=\"_blank\">https://wandb.ai/adithyabalagoni11-vasavi-college-of-engineering/paligemma_finetuning-003</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adithyabalagoni11-vasavi-college-of-engineering/paligemma_finetuning-003/runs/0l7us3rf' target=\"_blank\">https://wandb.ai/adithyabalagoni11-vasavi-college-of-engineering/paligemma_finetuning-003/runs/0l7us3rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/adithyabalagoni11-vasavi-college-of-engineering/paligemma_finetuning-003/runs/0l7us3rf?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7effb82e3730>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key='a2750455a136a8fa22fd4f037a3b5c5b68f3426f')\n",
    "#wandb logging parameters\n",
    "wandb.init(\n",
    "    project=\"paligemma_finetuning-003\",\n",
    "    config={\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"architecture\": \"florence_vqa-02\",\n",
    "    \"dataset\": \"amazon_entities\",\n",
    "    \"epochs\": 10,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:435: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5004' max='5004' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5004/5004 1:18:26, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.904200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.666700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.069100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.564200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.686800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.136900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.719100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.838800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.228900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.312800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.132800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.367100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.841100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.962000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.217000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.569600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.949600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.818800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.892800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.673700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.990400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.746100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.754100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.731600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.472600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.722200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.550500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.657400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.502800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.755300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.580500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.446100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.835300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.600200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.561100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.725200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.294700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.514100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.596300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.633400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.502100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.443900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.547700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.442900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.440100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.480500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.353600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.460500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.438600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.468900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.798400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.584000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.392300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.602700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.672900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.409300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.576000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.404000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.718700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.583700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.452200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.619300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.797000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.621600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.595600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.434700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.660200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.588000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.694600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.508000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.527200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.847000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.362200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.538000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.574800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.797100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.655200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.514700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>0.471100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.415400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>0.480900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.537700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>0.488700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.629000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.708300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.399400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>0.609000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.759200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>0.753200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.597400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>0.556400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.630300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>0.571400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.313200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.485000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.456900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>0.611800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.528900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>0.422800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.663200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5004, training_loss=0.7744381612629818, metrics={'train_runtime': 4708.3901, 'train_samples_per_second': 1.063, 'train_steps_per_second': 1.063, 'total_flos': 7.481653546575254e+16, 'train_loss': 0.7744381612629818, 'epoch': 1.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] ='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'\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=os.environ['HUGGING_FACE_HUB_TOKEN'])\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions[0].argmax(axis=-1)\n",
    "    \n",
    "    # Decode all predictions and labels\n",
    "    decoded_preds = []\n",
    "    decoded_labels = []\n",
    "    \n",
    "    for pred, label in zip(predictions, labels):\n",
    "        # Remove padding (-100) from labels\n",
    "        label = label[label != -100]\n",
    "        \n",
    "        # Decode and clean up\n",
    "        pred_text = processor.tokenizer.decode(pred, skip_special_tokens=True).strip()\n",
    "        label_text = processor.tokenizer.decode(label, skip_special_tokens=True).strip()\n",
    "        \n",
    "        decoded_preds.append(pred_text)\n",
    "        decoded_labels.append(label_text)\n",
    "    \n",
    "    # Initialize counters for F1 score calculation\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    true_negatives = 0\n",
    "    \n",
    "    # Calculate metrics based on the problem's criteria\n",
    "    for pred, gt in zip(decoded_preds, decoded_labels):\n",
    "        if pred != \"\" and gt != \"\":\n",
    "            if pred == gt:\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                false_positives += 1\n",
    "        elif pred != \"\" and gt == \"\":\n",
    "            false_positives += 1\n",
    "        elif pred == \"\" and gt != \"\":\n",
    "            false_negatives += 1\n",
    "        else:  # pred == \"\" and gt == \"\"\n",
    "            true_negatives += 1\n",
    "    \n",
    "    # Calculate precision and recall\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Compile metrics\n",
    "    metrics = {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1_score,\n",
    "        \"exact_match\": true_positives \n",
    "    }\n",
    "    \n",
    "    # Print some examples and metrics\n",
    "\n",
    "    \n",
    "    print(\"\\nExample Predictions:\")\n",
    "    import random\n",
    "    # Get 3 random indices\n",
    "    sample_indices = random.sample(range(len(decoded_preds)), min(50, len(decoded_preds)))\n",
    "    for idx in sample_indices:\n",
    "        print(f\"Predicted: {decoded_preds[idx]}\")\n",
    "        print(f\"Actual: {decoded_labels[idx]}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions[0].argmax(axis=-1)  # Get most likely token predictions\n",
    "    \n",
    "    decoded_preds = []\n",
    "    decoded_labels = []\n",
    "    \n",
    "    for pred, label in zip(predictions, labels):\n",
    "        # Remove padding and special tokens\n",
    "        label = label[label != -100]  # Remove padding\n",
    "        \n",
    "        # Decode using processor's tokenizer\n",
    "        pred_text = processor.decode(pred, skip_special_tokens=True).strip()\n",
    "        label_text = processor.decode(label, skip_special_tokens=True).strip()\n",
    "        \n",
    "        # Clean up any extra whitespace\n",
    "        pred_text = \" \".join(pred_text.split())\n",
    "        label_text = \" \".join(label_text.split())\n",
    "        \n",
    "        decoded_preds.append(pred_text)\n",
    "        decoded_labels.append(label_text)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        \"exact_match\": sum(p == l for p, l in zip(decoded_preds, decoded_labels)) / len(decoded_preds)\n",
    "    }\n",
    "    \n",
    "    # Print sample predictions\n",
    "    print(\"\\nSample Predictions:\")\n",
    "    for i in range(min(3, len(decoded_preds))):\n",
    "        print(f\"Prediction: {decoded_preds[i]}\")\n",
    "        print(f\"Label: {decoded_labels[i]}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "    return metrics    \n",
    "#\n",
    "# Setting up the training\n",
    "BATCH_SIZE=1\n",
    "NUM_EPOCHS=1\n",
    "from transformers import Trainer, TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    remove_unused_columns=False,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=1,\n",
    "    # evaluation_strategy=\"steps\",  # or \"epoch\"\n",
    "    # eval_steps=2,   \n",
    "    logging_steps=50,\n",
    "    save_steps=100,\n",
    "    save_total_limit=1,\n",
    "\n",
    "\n",
    "           # if using \"steps\"\n",
    "    warmup_steps=2,\n",
    "    learning_rate=5e-6,\n",
    "    weight_decay=1e-6,\n",
    "    adam_beta2=0.999,\n",
    "    optim=\"adamw_hf\",\n",
    "    save_strategy=\"steps\",\n",
    "    push_to_hub=True,\n",
    "    output_dir=\"pali-gemma-ft-ml-challenge\",\n",
    "    bf16=True,\n",
    "    report_to=[\"wandb\"],\n",
    "    dataloader_pin_memory=False\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    # eval_dataset=val_dataset,  # Add validation dataset\n",
    "    data_collator=collate_fn,\n",
    "    # compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/adithyabalagoni11/pali-gemma-ft-ml-challenge/commit/052daaacde27d3b630be5c88c4c8a392669a2676', commit_message='End of training', commit_description='', oid='052daaacde27d3b630be5c88c4c8a392669a2676', pr_url=None, repo_url=RepoUrl('https://huggingface.co/adithyabalagoni11/pali-gemma-ft-ml-challenge', endpoint='https://huggingface.co', repo_type='model', repo_id='adithyabalagoni11/pali-gemma-ft-ml-challenge'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions[0].argmax(axis=-1)\n",
    "    \n",
    "    # Decode all predictions and labels\n",
    "    decoded_preds = []\n",
    "    decoded_labels = []\n",
    "    \n",
    "    for pred, label in zip(predictions, labels):\n",
    "        # Remove padding (-100) from labels\n",
    "        label = label[label != -100]\n",
    "        \n",
    "        # Decode and clean up\n",
    "        pred_text = processor.tokenizer.decode(pred, skip_special_tokens=True).strip()\n",
    "        label_text = processor.tokenizer.decode(label, skip_special_tokens=True).strip()\n",
    "        \n",
    "        decoded_preds.append(pred_text)\n",
    "        decoded_labels.append(label_text)\n",
    "    \n",
    "    # Initialize counters for F1 score calculation\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    true_negatives = 0\n",
    "    \n",
    "    # Calculate metrics based on the problem's criteria\n",
    "    for pred, gt in zip(decoded_preds, decoded_labels):\n",
    "        if pred != \"\" and gt != \"\":\n",
    "            if pred == gt:\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                false_positives += 1\n",
    "        elif pred != \"\" and gt == \"\":\n",
    "            false_positives += 1\n",
    "        elif pred == \"\" and gt != \"\":\n",
    "            false_negatives += 1\n",
    "        else:  # pred == \"\" and gt == \"\"\n",
    "            true_negatives += 1\n",
    "    \n",
    "    # Calculate precision and recall\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Compile metrics\n",
    "    metrics = {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1_score,\n",
    "        \"exact_match\": true_positives \n",
    "    }\n",
    "    \n",
    "    # Print some examples and metrics\n",
    "\n",
    "    \n",
    "    print(\"\\nExample Predictions:\")\n",
    "    import random\n",
    "    # Get 3 random indices\n",
    "    # sample_indices = random.sample(range(len(decoded_preds)), min(3, len(decoded_preds)))\n",
    "    for idx in sample_indices[:200]:\n",
    "        print(f\"Predicted: {decoded_preds[idx]}\")\n",
    "        print(f\"Actual: {decoded_labels[idx]}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "trainer.compute_metrics=new_compute_metrics\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add progress bar manually\n",
    "print(\"Evaluating on test set...\")\n",
    "with tqdm(total=len(test_dataset)) as pbar:\n",
    "    test_results = trainer.evaluate(\n",
    "        eval_dataset=test_dataset,\n",
    "        metric_key_prefix=\"test\"\n",
    "    )\n",
    "    pbar.update(len(test_dataset))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
